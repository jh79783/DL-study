# 합성곱 신경망 (Convolution Neural Network)

합성곱 신경망은 저수준의 패턴(edge, blob 등)이 조합되어 복잡한 패턴(texture, object 등)에 반응한다는 사실을 통해 고안된 아이디어이다.

5장과 6장에서 모든 이미지의 픽셀이 연결되어 있는 fully connected layer를 사용하였다.
들어온 이미지를 affine으로 명시된 fully-connected 연산과 ReLU와같은 비선형 함수의 계층을 여러층 쌓은 구조이다.

![fully connect](..\img\fully connect.jpg)

이번 7장에서 사용할 CNN의 구조는 합성곱 계층과 풀링층이라고 하는 새로운 계층을 fully-connected 앞에 추가 함으로 써 연산이 수행하도록 한다.

![cnn connect](..\img\cnn connect.jpg)

합성곱 계층은 이미지에 필터링 기법(합성곱 연산)이 적용되며, 풀링 계층은 이미지의 국소적인 부분을 대표스칼라 값으로 변환하여 이미지의 크기를 줄여주는 역할을 한다. 풀링에도 여러기법이 있지만, 이미지 인식에서는 최대풀링과 평균풀링을 주로 사용한다.



5장에서 우리는 입력데이터를 1차원으로 바꾸어 넣었다. 

> 입력데이터 = 가로\*세로\*채널 

앞의 공식에 의해 앞서 사용한 flower data는 100\*100\*3=30000개를 입력데이터로 사용하였다. 이것이 fully conneceted layer인데, 이것의 문제점은 **데이터의 형상이 무시**된다는 것이다. (같은 차원의 뉴런으로 취급)
이미지 데이터는 3차원 공간에서 다음과 같은 많은 정보가 내포되어 있다.

> RGB채널의 연관성
> 공간적으로 가까운 픽셀과의 연관성
> 거리가 먼 픽셀과의 무연관성 등

1차원으로 바꾸어 펼쳐주게되면 이러한 정보가 모두 무시된다.
입력데이터의 형상을 유지시켜주기 위해 CNN에서는 이미지 데이터의 3차원을 그대로 입력층에 입력받게 된다.
이때 사용하는 이미지 데이터는 **(높이,넓이,채널)**의 3차원인 텐서(tensor)로 표현을 한다.

개념적으로는 3차원 텐서지만 현실적으로는 병렬 연산을 위해 **(배치,높이,넓이,채널)**인4차원 텐서를 이용한다고한다.



## 합성곱 연산

합성곱 연산을 통하여 feature map을 출력한다.
합성곱층에서는 커널 혹은 필터라고 불리는 n\*m의 매트릭스를 사용한다.
즉, 이 커널이 합성곱층에서의 가중치 파라미터 Weight에 해당하게 되며 이를 통해 feature map을 출력하여 다음 층으로 전달한다.

![합성곱 연산](..\img\합성곱 연산.jpg)

합성곱 연산을 진행하게 되면 출력 이미지가 입력 이미지보다 작아지게 되는데, 이것이 여러 층을 통과하게 되면 가장자리에 있는 픽셀의 정보는 사라지게 된다.이러한 문제를 해결하는 것이 패딩(padding)이다.
패딩을 할 때 채울 값을 지정할 수 있지만, 주로 0으로 채워 사용하게 된다(zero-padding).
패딩 방식에는 SAME과 VALID방식이 있다.
SAME패딩은 입력과 같은 크기의 출력이 만들어지도록 하고, VALID패딩은 출력이 입력보다 적어지게 되지만 출력 픽셀 계산에 유용한 픽셀들만 이용된다. 
보통 SAME패딩이 구조를 설계하기 쉽고, 반복하기도 유리하기 때문에 SAME패딩이 더 널리 사용된다.

입력 픽셀로부터 출력 픽셀을 생성하는데 이때 일정한 간격을 건너뛰며 생성할 수 있다. 즉, 출력계층의 데이터 크기를 조절할 수 있다는 것이다. 이것을 스트라이드(Stride) or 건너뛰기 라고 한다.
스트라이드는 이미지 크기를 줄여 이미지를 처리하는데 부담을 줄여주지만 그만큼 정보를 무시하게 된다. 따라서 합성곱층에서보다 풀링계층에서 더 많이 사용된다.

## 풀링

평균값 풀링의 경우 모든 입력 픽셀값이 출력에 반영된다. 그에 비해 최대값 풀링의 경우 반영되지 않는 입력 픽셀이 생기는데 이들은 합성곱 계층에서의 스트라이드 처럼 무시당하지 않는다. 왜냐하면 최대값을 뽑는 과정에서 간접적으로 영향을 미치기 때문이다.

## 채널과 커널의 확장

이미지에는 다양한 패턴과 특징이 있기때문에 한 가지의 feature map으로 충분하지 않다. 즉, 하나의 커널로 충분하지 않다는 것이다.
따라서 차원이라는 채널을 추가하여 더 고수준의 패턴과 특징을 출력할 수 있다.

합성곱 계층에서 입력 채널 수는 입력 데이터나 이전에 입력받은 데이터의 채널수로 정해지게 되는데, 출력 채널수는 설계자가 마음대로 정할 수 있다. 보통 출력에서 채널수를 늘려 더 많은 특징과 패턴을 출력하게 된다.
하지만 이렇게 하면 정보량은 계속해서 많아지게 되는데 이를 풀링 계층에서 다시 줄여준다.

따라서 커널은 입력 데이터의 채널수와 같은 차원수를 갖고있어야 한다. 또한 출력 채널수만큼 만들어야 하기 때문에 결국 커널은 (커널크기, 채널수, 출력 영상의 채널수) 를 갖고있는 4차원 구조가 되어야 한다.

## 역전파 처리

채널을 제외한 높이와 너비만을 고려한 2차원에서의 역전파(Backpropagation)을 알아보자.

fully-connected layer에서 매트릭스를 통해 순전파와 역전파를 진행하였는데 합성곱에서도 이와 비슷하므로 매트릭스 연산을 통해 진행한다.

스트라이드는 1로 설정되어있다고 가정하였을때, CNN의 I번째 합성곱 계층의 출력 $X^{(I)}$의 K번째 채널에 대한 i번째 행, j번째 열은 다음과 같이 계산된다.
$$
X^{(I)}_{k,i,j}=\sum^{F^{(I)}_H-1}_{m=0}\sum^{F^{(I)}_H-1}_{n=0}W^{(I)}_{k,m,n}f(X^{(I-1)}_{k,(i+m),(j+n)})+b^{(I)}_k
$$
W는 I번째 합성곱 계층의 k번째 필터를 의미하고, F는 각각 I번째 합성곱 계층 커널의 높이와 너비, f는 비선형 활성화 함수를 나타낸다.
합성곱 연산시 (i,j)에 (m,n)을 더해가며 커널을 적용한다. 이는 합성곱이 아니라 상호 상관(corss-correlation)이라고 한다. 하지만 CNN에서는 필터 자체를 학습시키기 때문에 합성곱과 상호 상관을 구분하지 않는다.

경사하강법(gradient descent method)를 이용한 **W**는 다음과 같이 계산된다.
$$
\bold{W}=\bold{W}-\eta\frac{\partial{L}}{\partial{\bold{W}}}
$$
CNN에서의 가중치는 합성곱 계층의 필터에 해당하기 때문에 이는 필터들의 각 요소를 학습하는 것과 같은 의미이다. 
따라서 I번째 합성곱 계층에서 k번째 필터의 i,j번째 요소인 $\bold{W}^{(I)}_{k,i,j}$에 대해 손실 함수 L을 편미분 한 공식은 다음과 같다.
$$
\frac{\partial{L}}{\partial{\bold{W}^{(I)}_{k,i,j}}}=\sum^{\bold{X}^{(I-1)}_H-F^{(I)}_H}_{m=0}\sum^{\bold{X}^{(I-1)}_W-F^{(I)}_W}_{n=0}\frac{\partial{L}}{\partial{\bold{X}^{(I)}_{k,m,n}}}\frac{\partial{\bold{X}^{(I)}_{k,m,n}}}{\partial{\bold{W}^{(I)}_{k,i,j}}}\\=\sum^{\bold{X}^{(I-1)}_H-F^{(I)}_H}_{m=0}\sum^{\bold{X}^{(I-1)}_W-F^{(I)}_W}_{n=0}\delta^{I}_{k,m,n}\frac{\partial{\bold{X}^{(I)}_{k,m,n}}}{\partial{\bold{W}^{(I)}_{k,i,j}}}
$$
$\bold{X}^{(I-1)}_H,\bold{X}^{(I-1)}_W$는 I-1번째 합성곱 층에서 출력된 이미지의 높이와 너비를 의미한다.
$$
\frac{\partial{\bold{X}}^{(I)}_{k,m,n}}{\partial{\bold{W}^{(I)}_{k,i,j}}}=\frac{\partial}{\partial{\bold{W}^{(I)}_{k,i,j}}}\{\sum^{F^{(I)}_{H}-1}_{p=0}\sum^{F^{(I)}_W-1}\bold{W}^{(I)}_{k.p,q}f(\bold{X}^{(I-1)}_{k,(m+p),(n+q)}+b^{(I)}_k)
$$
미분과정에서 $\bold{W}^{(I)}_{k,i,j}$와 독립적인 항은 모두 0이 되어 p=i, q=j인 항만 고려하면 식은 다음과 같다.
$$
\frac{\partial{\bold{X}}^{(I)}_{k,m,n}}{\partial{\bold{W}^{(I)}_{k,i,j}}}=\frac{\partial}{\partial{\bold{W}^{(I)}_{k,i,j}}}\{\bold{W}^{(I)}_{k,i,j}f(\bold{X}^{(I-1)}_{k,(i+m),(j,n)})\}\\=f(\bold{X}^{(I-1)}_{k,(i+m),(j+n)})
$$

$$
\delta^{I}_{k,m,n}=\frac{\partial{L}}{\partial{\bold{X}^{(I)}_{k,m,n}}}\\=\sum^{F^{(I+1)}_{H}-1}_{p=0}\sum^{F^{(I+1)}_{W}-1}_{q=0}\frac{\partial{L}}{\partial{\bold{X}^{(I+1)}_{k,(m-p),(n-q)}}}\frac{\partial\bold{X}^{(I+1)}_{k,(m-p),(n-q)}}{\partial\bold{X}^{(I)}_{k,m,n}}\\=\sum^{F^{(I+1)}_H-1}_{p=0}\sum^{F^{(I+1)}_W-1}_{q=0}\delta^{(I+1)}_{k,(m-p),(n-q)}\frac{\partial\bold{X}^{(I+1)}_{k,(m-p),(n-q)}}{\partial{\bold{X}^{(I)}_{k,m,n}}}\\=\sum^{F^{(I+1)}_H-1}_{p=0}\sum^{F^{(I+1)}_W-1}_{q=0}\delta^{(I+1)}_{k,(m-p),(n-q)}\frac{\partial}{\partial\bold{X}^{(I)}_{k,m,n}}\{\sum^{F^{(I+1)}_H-1}_{r=0}\sum^{F^{(I+1)}_W-1}_{s=0}\bold{W}^{(I+1)}_{k,r,s}f(\bold{X}^{(I)}_{k,(m-p+r),(n-q+s)})\}\\m-p+r=m(r=p),n-q+s=n(s=q)항\\threfore\\\frac{\partial\bold{X}^{(I+1)}_{k,(m-p),(n-q)}}{\partial{\bold{X}^{(I)}_{k,m,n}}}=\frac{\partial}{\partial\bold{X}^{(I)}_{k,p,q}}\{\bold{W}^{(I+1)}_{k,p,q}f(\bold{X}^{(I)}_{k,m,n})\}\\=\bold{W}^{(I+1)}_{k,p,q}f'(\bold{X}^{I}_{k,m,n})\\\delta^{I}_{k,m,n}=\sum^{F^{(I+1)}_H-1}_{p=0}\sum^{F^{(I+1)}_W-1}_{q=0}\delta^{(I+1)}_{k,(m-p),(n-q)}\bold{W}^{(I+1)}_{k,p,q}f'(\bold{X}^{I}_{k,m,n})
$$

따라서 최종적으로 $\bold{W}^{(I)}_{k,i,j}$에 대한 L의 gradient는5 다음과 같다.
$$
\sum^{\bold{X}^{(I-1)}_H-F^{(I)}_H}_{m=0}\sum^{\bold{X}^{(I-1)}_W-F^{(I)}_W}_{n=0}\delta^{I}_{k,m,n}\frac{\partial{\bold{X}^{(I)}_{k,m,n}}}{\partial{\bold{W}^{(I)}_{k,i,j}}}\\=\sum^{\bold{X}^{(I-1)}_H-F^{(I)}_H}_{m=0}\sum^{\bold{X}^{(I-1)}_W-F^{(I)}_W}_{n=0}(\sum^{F^{(I+1)}_H-1}_{p=0}\sum^{F^{(I+1)}_W-1}_{q=0}\delta^{(I+1)}_{k,(m-p),(n-q)}\bold{W}^{(I+1)}_{k,p,q}f'(\bold{X}^{I}_{k,m,n}))(f(\bold{X}^{(I-1)}_{k,(i+m),(j+n)}))
$$
위의 공식으로 W를 update하게 되며 Bias또한 이와 같이 update하게 된다.

이 공식은 (높이,넓이)인 2차원의 합성곱 연산을 한 것이다.
하지만 실제로 우리는 (넓이,높이,채널)인 3차원의 합성곱을 진행한다. 3차원의 합성곱 연산은 자세히 적지 않고 연산 방법만 적도록 하겠다.

먼저 3차원의 일반적인 공식이다.
$$
y_{n,r,c,m}=\sum^{kh}_{i=1}\sum^{kw}_{j=1}\sum^{xchn}_{k=1}k_{i,j,k,m}x_{n,r+i-bh,c+j-bw,k}
$$
앞부터 차례로 이미지의 h,w,ch에 대한 합성곱 연산식이다.

위의 식을 구현하기 위해 7종 반복문을 사용한다. 하지만 이것은 시간이 오래걸리기 때문에 부적절하다. 

다음 방법으로는 중복에 이용되는 요소들을 전처리를 추가하여 미리 계산하는 것이다. 이를 사용하면 7중 반복문에서 5중 반복문으로 줄어들게 되지만, 이 역시 커다란 효과를 기대하기는 어렵다.

따라서 마지막방법인 행렬을 통한 계산으로 진행한다.
두개의 커다란 행렬에 계산 대상들을 넣고 한번의 행렬 곱셉으로 대부분의 계산을 완료할 수 있다. 이것은 위의 두 방법보다 복잡하며, 전처리와 후처리 과정을 거쳐야 하지만 이 방법을 사용하게 되면 수백에서 수천배까지 빠른 속도로 처리된다.

## CNN의 일반적인 배치 구성

CNN은 보통 합성곱 계층과 풀링계층을 교대로 배치하여 구성한다.
이러한 구성은 합성곱 계층의 이미지 해상도는 유지하면서 채널 수는 늘리게 되고, 풀링계층에서는 채널 수를 유지하며 해상도를 줄이게 된다.
그 후 최종적으로 fully-connected layer를 추가하여 출력을 한다.
따라서 간단히 보면 구성은 다음과 같아진다.

> 입력 데이터 -> 합성곱 계층 -> 풀링 계층 -> fully-connected 계층 -> 최종 출력

하지만 풀링계층의 출력은 4차원 텐서 형태이지만 fully-connected 계층에서는 2차원의 행렬이 필요하다. 따라서 풀링계층에서의 출력을 데이터별 벡터 형태의 2차원 행렬 입력으로 바꾸어야 한다.

이를 위해 미니배치 데이터 축을 기준으로 나머지 세 축을 한 차원으로 축소 시킨다.